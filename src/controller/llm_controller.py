# controller/llm_controller.py

from clients.llm.llm_client import LLMClient
from clients.logging.logger import logger
import time
import sys

class LLMController:
    def __init__(self, config):
        self.llm_client = LLMClient()
        self.retry_time = config.retry_config.retry_max_time
        self.retry_interval = config.retry_config.retry_interval_time

    def fix_code_with_llm(self, prompt):
        """éæµå¼ä¿®å¤ä»£ç ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        for attempt in range(self.retry_time):
            try:
                result = self.llm_client.fix_code(prompt)
                logger.info(f"LLMå“åº”æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(result)}")
                return result
            except Exception as e:
                logger.warning(f"LLMè¯·æ±‚å¤±è´¥: {e}, retry {attempt+1}/{self.retry_time}")
                time.sleep(self.retry_interval)
        logger.error("LLMä¿®å¤è¯·æ±‚å¤šæ¬¡å¤±è´¥ï¼Œç»ˆæ­¢ã€‚")
        return ""

    def fix_code_with_llm_stream(self, prompt):
        """æµå¼ä¿®å¤ä»£ç """
        try:
            logger.info("å¼€å§‹æµå¼LLMä¿®å¤...")
            print(f"ğŸ¤– AIåˆ†æå¼€å§‹...", flush=True)
            
            full_response = ""
            for chunk in self.llm_client.fix_code_stream(prompt):
                full_response += chunk
                # å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œç¡®ä¿workflow executorèƒ½æ•è·
                print(chunk, end="", flush=True)
                sys.stdout.flush()
            
            print(f"\nğŸ¤– AIåˆ†æå®Œæˆï¼Œå“åº”é•¿åº¦: {len(full_response)}", flush=True)
            logger.info(f"æµå¼LLMå“åº”æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(full_response)}")
            return full_response
            
        except Exception as e:
            error_msg = f"æµå¼LLMè¯·æ±‚å¤±è´¥: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}", flush=True)
            return ""

    def analyze_pipeline_logs(self, logs):
        """åˆ†æPipelineæ—¥å¿—"""
        try:
            logger.info("å¼€å§‹åˆ†æPipelineæ—¥å¿—...")
            print(f"ğŸ” å¼€å§‹åˆ†æPipelineæ—¥å¿—...", flush=True)
            
            full_response = ""
            for chunk in self.llm_client.analyze_pipeline_logs(logs):
                full_response += chunk
                # å®æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
                print(chunk, end="", flush=True)
                sys.stdout.flush()
            
            print(f"\nâœ… æ—¥å¿—åˆ†æå®Œæˆ", flush=True)
            logger.info(f"Pipelineæ—¥å¿—åˆ†æå®Œæˆï¼Œå“åº”é•¿åº¦: {len(full_response)}")
            return full_response
            
        except Exception as e:
            error_msg = f"Pipelineæ—¥å¿—åˆ†æå¤±è´¥: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}", flush=True)
            return ""