# controller/llm_controller.py

from clients.llm.llm_client import LLMClient
from clients.logging.logger import logger
import time
import sys
from typing import Optional, List

class LLMController:
    def __init__(self, config):
        self.llm_client = LLMClient()
        self.retry_time = config.retry_config.retry_max_time
        self.retry_interval = config.retry_config.retry_interval_time
        self.available_models = self.llm_client.get_available_models()
        self.current_model_index = 0
        
        logger.info(f"LLMæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ¨¡å‹: {self.available_models}")
        print(f"ğŸ¤– å¯ç”¨LLMæ¨¡å‹: {', '.join(self.available_models)}", flush=True)

    def get_current_model(self) -> Optional[str]:
        """
        è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹
        
        Returns:
            Optional[str]: å½“å‰æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹åˆ™è¿”å›None
        """
        if self.current_model_index < len(self.available_models):
            return self.available_models[self.current_model_index]
        return None

    def switch_to_next_model(self) -> bool:
        """
        åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
        """
        self.current_model_index += 1
        if self.current_model_index < len(self.available_models):
            current_model = self.available_models[self.current_model_index]
            logger.info(f"åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {current_model} (ç´¢å¼•: {self.current_model_index})")
            print(f"ğŸ”„ åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹: {current_model}", flush=True)
            return True
        else:
            logger.error("å·²ç»å°è¯•äº†æ‰€æœ‰å¯ç”¨æ¨¡å‹")
            print("âŒ å·²ç»å°è¯•äº†æ‰€æœ‰å¯ç”¨æ¨¡å‹", flush=True)
            return False

    def reset_model_index(self):
        """
        é‡ç½®æ¨¡å‹ç´¢å¼•åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹
        """
        self.current_model_index = 0
        if self.available_models:
            logger.info(f"é‡ç½®åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹: {self.available_models[0]}")
            print(f"ğŸ”„ é‡ç½®åˆ°ç¬¬ä¸€ä¸ªæ¨¡å‹: {self.available_models[0]}", flush=True)

    def has_more_models(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ¨¡å‹å¯ä»¥å°è¯•
        
        Returns:
            bool: æ˜¯å¦è¿˜æœ‰æ›´å¤šæ¨¡å‹
        """
        return self.current_model_index < len(self.available_models) - 1

    def fix_code_with_llm(self, prompt):
        """éæµå¼ä¿®å¤ä»£ç ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        current_model = self.get_current_model()
        if not current_model:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return ""

        for attempt in range(self.retry_time):
            try:
                result = self.llm_client.fix_code(prompt, current_model)
                logger.info(f"LLMå“åº”æˆåŠŸï¼Œæ¨¡å‹: {current_model}ï¼Œå“åº”é•¿åº¦: {len(result)}")
                return result
            except Exception as e:
                logger.warning(f"LLMè¯·æ±‚å¤±è´¥ï¼Œæ¨¡å‹: {current_model}ï¼Œé”™è¯¯: {e}, retry {attempt+1}/{self.retry_time}")
                time.sleep(self.retry_interval)
        logger.error(f"LLMä¿®å¤è¯·æ±‚å¤šæ¬¡å¤±è´¥ï¼Œæ¨¡å‹: {current_model}ï¼Œç»ˆæ­¢ã€‚")
        return ""

    def fix_code_with_llm_stream(self, prompt):
        """æµå¼ä¿®å¤ä»£ç ï¼ŒåŠ¨æ€æ˜¾ç¤ºåˆ†æè¿›åº¦"""
        current_model = self.get_current_model()
        if not current_model:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹", flush=True)
            return ""

        try:
            logger.info(f"å¼€å§‹æµå¼LLMä¿®å¤ï¼Œä½¿ç”¨æ¨¡å‹: {current_model}")
            print(f"ğŸ¤– AIåˆ†æå¼€å§‹ï¼Œä½¿ç”¨æ¨¡å‹: {current_model}...", flush=True)
            
            full_response = ""
            line_count = 0
            
            for chunk in self.llm_client.fix_code_stream(prompt, current_model):
                full_response += chunk
                
                # è®¡ç®—å½“å‰è¡Œæ•°ï¼ˆé€šè¿‡æ¢è¡Œç¬¦è®¡ç®—ï¼‰
                new_line_count = full_response.count('\n')
                if new_line_count > line_count:
                    line_count = new_line_count
                    # åŠ¨æ€æ›´æ–°åˆ†æè¿›åº¦
                    print(f"\rğŸ¤– æ­£åœ¨åˆ†æç¬¬ {line_count} è¡Œä»£ç ...", end="", flush=True)
                
                sys.stdout.flush()
            
            # å®Œæˆåæ¢è¡Œå¹¶æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            print(f"\nğŸ¤– AIåˆ†æå®Œæˆï¼Œæ¨¡å‹: {current_model}ï¼Œå…±åˆ†æ {line_count} è¡Œï¼Œå“åº”é•¿åº¦: {len(full_response)}", flush=True)
            logger.info(f"æµå¼LLMå“åº”æˆåŠŸï¼Œæ¨¡å‹: {current_model}ï¼Œå“åº”é•¿åº¦: {len(full_response)}")
            return full_response
            
        except Exception as e:
            error_msg = f"æµå¼LLMè¯·æ±‚å¤±è´¥ï¼Œæ¨¡å‹: {current_model}ï¼Œé”™è¯¯: {e}"
            logger.error(error_msg)
            print(f"\nâŒ {error_msg}", flush=True)
            return ""

    def fix_code_with_all_models(self, prompt):
        """
        å°è¯•æ‰€æœ‰å¯ç”¨æ¨¡å‹è¿›è¡Œä»£ç ä¿®å¤ï¼Œç›´åˆ°æˆåŠŸæˆ–æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
        
        Args:
            prompt: ä¿®å¤æç¤ºè¯
            
        Returns:
            tuple: (success: bool, response: str, used_model: str)
        """
        original_index = self.current_model_index
        
        try:
            # ä»å½“å‰æ¨¡å‹å¼€å§‹å°è¯•æ‰€æœ‰æ¨¡å‹
            while True:
                current_model = self.get_current_model()
                if not current_model:
                    break
                
                logger.info(f"å°è¯•ä½¿ç”¨æ¨¡å‹: {current_model}")
                print(f"ğŸ¤– å°è¯•ä½¿ç”¨æ¨¡å‹: {current_model}", flush=True)
                
                response = self.fix_code_with_llm_stream(prompt)
                
                if response and response.strip():
                    logger.info(f"æ¨¡å‹ {current_model} æˆåŠŸç”Ÿæˆå“åº”")
                    print(f"âœ… æ¨¡å‹ {current_model} æˆåŠŸç”Ÿæˆå“åº”", flush=True)
                    return True, response, current_model
                else:
                    logger.warning(f"æ¨¡å‹ {current_model} è¿”å›ç©ºå“åº”")
                    print(f"âš ï¸ æ¨¡å‹ {current_model} è¿”å›ç©ºå“åº”", flush=True)
                
                # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ¨¡å‹
                if not self.switch_to_next_model():
                    break
            
            # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥äº†
            logger.error("æ‰€æœ‰å¯ç”¨æ¨¡å‹éƒ½æ— æ³•ç”Ÿæˆæœ‰æ•ˆå“åº”")
            print("âŒ æ‰€æœ‰å¯ç”¨æ¨¡å‹éƒ½æ— æ³•ç”Ÿæˆæœ‰æ•ˆå“åº”", flush=True)
            return False, "", ""
            
        finally:
            # æ¢å¤åŸå§‹æ¨¡å‹ç´¢å¼•
            self.current_model_index = original_index

    def analyze_pipeline_logs(self, logs):
        """åˆ†æPipelineæ—¥å¿—ï¼Œä½¿ç”¨åŠ¨æ€è¿›åº¦æ˜¾ç¤º"""
        current_model = self.get_current_model()
        if not current_model:
            logger.error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹", flush=True)
            return ""

        try:
            logger.info(f"å¼€å§‹åˆ†æPipelineæ—¥å¿—ï¼Œä½¿ç”¨æ¨¡å‹: {current_model}")
            print(f"ğŸ” å¼€å§‹åˆ†æPipelineæ—¥å¿—ï¼Œä½¿ç”¨æ¨¡å‹: {current_model}...", flush=True)
            
            full_response = ""
            line_count = 0
            
            for chunk in self.llm_client.analyze_pipeline_logs(logs, current_model):
                full_response += chunk
                
                # è®¡ç®—å½“å‰è¡Œæ•°
                new_line_count = full_response.count('\n')
                if new_line_count > line_count:
                    line_count = new_line_count
                    print(f"\rğŸ” æ­£åœ¨åˆ†æç¬¬ {line_count} è¡Œæ—¥å¿—...", end="", flush=True)
                
                sys.stdout.flush()
            
            print(f"\nâœ… æ—¥å¿—åˆ†æå®Œæˆï¼Œæ¨¡å‹: {current_model}ï¼Œå…±åˆ†æ {line_count} è¡Œ", flush=True)
            logger.info(f"Pipelineæ—¥å¿—åˆ†æå®Œæˆï¼Œæ¨¡å‹: {current_model}ï¼Œå“åº”é•¿åº¦: {len(full_response)}")
            return full_response
            
        except Exception as e:
            error_msg = f"Pipelineæ—¥å¿—åˆ†æå¤±è´¥ï¼Œæ¨¡å‹: {current_model}ï¼Œé”™è¯¯: {e}"
            logger.error(error_msg)
            print(f"\nâŒ {error_msg}", flush=True)
            return ""